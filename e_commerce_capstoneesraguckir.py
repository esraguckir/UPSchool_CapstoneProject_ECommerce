# -*- coding: utf-8 -*-
"""E-commerce_CapstoneEsraGuckir.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PRnluhnN_HXGK5OFQtYUBEMZmqeAKiAU

#**BRAZILIAN E-COMMERCE DATA: Exploratory Data Analysis (EDA)**

ðŸ›’ ðŸ’»

Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline
import seaborn as sns
from PIL import Image
sns.set()

from datetime import date
from scipy.stats import pearsonr

pip install matplotlib==3.1.3

"""Loading Data"""

from google.colab import drive
drive.mount('/content/drive')

customers_df = pd.read_csv('/content/drive/MyDrive/capstone datasets/olist_customers_dataset.csv')
geolocation_df = pd.read_csv('/content/drive/MyDrive/capstone datasets/olist_geolocation_dataset.csv')
orders_df = pd.read_csv('/content/drive/MyDrive/capstone datasets/olist_orders_dataset.csv')
order_items_df = pd.read_csv('/content/drive/MyDrive/capstone datasets/olist_order_items_dataset.csv')
order_payments_df = pd.read_csv('/content/drive/MyDrive/capstone datasets/olist_order_payments_dataset.csv')
order_reviews_df = pd.read_csv('/content/drive/MyDrive/capstone datasets/olist_order_reviews_dataset.csv')
products_df = pd.read_csv('/content/drive/MyDrive/capstone datasets/olist_products_dataset.csv')
sellers_df = pd.read_csv('/content/drive/MyDrive/capstone datasets/olist_sellers_dataset.csv')
category_translations_df = pd.read_csv('/content/drive/MyDrive/capstone datasets/product_category_name_translation.csv')

"""##**Data Descriptions**

**Data Schema**
"""

data_schema = Image.open('/content/dataschema.png')
data_schema.thumbnail((700, 700))
data_schema

"""**Check the size of each dataframe**"""

df_names = ['customers_df','geolocation_df', 'orders_df', 'order_items_df','order_payments_df',
            'order_reviews_df','products_df','sellers_df','category_translations_df']
for df in df_names:
    print("Dataset {} has shape {}".format(df, eval(df).shape))

"""**Take a look at to first 5 rows of dataframes**"""

customers_df.head()

geolocation_df.head()

orders_df.head()

order_items_df.head()

order_payments_df.head()

order_reviews_df.head()

products_df.head()

sellers_df.head()

category_translations_df.head()

"""**Descriptions of the columns in each dataframe**"""

col_desc = Image.open('/content/coldescriptions.png')
col_desc.thumbnail((1000, 1800))
col_desc

"""##**Data Exploration**

**Check the null values**
"""

for df in df_names:
    print("In {} there are {} null values".format(df, eval(df).isnull().sum().sum()))

"""Orders, order_reviews and products datasets have missing values.

**Deal with null values**

For orders dataset:
"""

orders_df.isnull().sum()

"""Order approval date, carrier delivery date and customer delivery date columns have missing values.

*   Order approval is usually done in a short time after the purchase of the order, so missing approval timestamp could be substituted with the purchase timestamp.
*   It is seen that the carrier delivery timestamp is often same as the order approval date, so the carrier delivery date could be substituted with the approval date.

*   Estimated delivery date is often later than the actual customer delivery date, so the missing customer delivery date could be subtituted with the estimated delivery date.

"""

orders_df["order_approved_at"] = orders_df["order_approved_at"].fillna(orders_df["order_purchase_timestamp"])
orders_df["order_delivered_carrier_date"] = orders_df["order_delivered_carrier_date"].fillna(orders_df["order_approved_at"])
orders_df["order_delivered_customer_date"] = orders_df["order_delivered_customer_date"].fillna(orders_df["order_estimated_delivery_date"])

orders_df.isnull().sum()

"""No more missing values in orders dataset.

For products dataset:
"""

products_df.isnull().sum()

"""Check the rows where the size information is missing"""

products_df[products_df['product_weight_g'].isnull()]

"""Drop those 2 rows. """

products_df = products_df[~(products_df['product_weight_g'].isnull())]

"""Check the rows where the product name is missing"""

products_df[products_df['product_name_lenght'].isnull()].head()

"""Drop those 610 rows because no useful information can be gathered with just size and product ID."""

products_df = products_df[~(products_df['product_name_lenght'].isnull())]

products_df.isnull().sum()

"""No more missing values in products dataset.

For order reviews dataset:
"""

order_reviews_df.isnull().sum()

"""Drop the review_comment_title column"""

order_reviews_df = order_reviews_df.drop(['review_comment_title'],axis =1 )

""" Replace missing review messages with string 'NONE'"""

order_reviews_df['review_comment_message'] = order_reviews_df['review_comment_message'].fillna('NONE')

order_reviews_df.isnull().sum()

"""No more missing values in order reviews dataset.

All missing values are handled.

**Merge all the datasets**
"""

df = pd.merge(orders_df,order_payments_df, on="order_id")
df = pd.merge(df,customers_df, on="customer_id")
df = pd.merge(df,order_items_df, on="order_id")
df = pd.merge(df,sellers_df, on="seller_id")
df = pd.merge(df,order_reviews_df, on="order_id")
df = pd.merge(df,products_df, on="product_id")
df = pd.merge(df,category_translations_df, on="product_category_name")

df.shape

df.head()

"""**Check if duplicated rows exist**"""

df[df.duplicated(keep='first')].shape

"""**Check the data types**"""

df.info()

"""**Convert the type of date columns to datetime**"""

df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])
df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])
df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])
df['review_creation_date'] = pd.to_datetime(df['review_creation_date'])
df['review_answer_timestamp'] = pd.to_datetime(df['review_answer_timestamp'])

df['order_purchase_year'] = df.order_purchase_timestamp.apply(lambda x: x.year)
df['order_purchase_month'] = df.order_purchase_timestamp.apply(lambda x: x.month)
df['monthyear'] = pd.to_datetime(df['order_purchase_timestamp']).dt.to_period('M')
df['order_purchase_dayofweek'] = df.order_purchase_timestamp.apply(lambda x: x.dayofweek)
df['order_purchase_hour'] = df.order_purchase_timestamp.apply(lambda x: x.hour)
df['order_purchase_day'] = df['order_purchase_dayofweek'].map({0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'})
df['order_purchase_mon'] = df.order_purchase_timestamp.apply(lambda x: x.month).map({0:'Jan',1:'Feb',2:'Mar',3:'Apr',4:'May',5:'Jun',6:'Jul',7:'Aug',8:'Sep',9:'Oct',10:'Nov',11:'Dec'})

"""**Descriptive statistics of dataframe**"""

df.describe()

"""**Check correlation between attributes**"""

f, ax = plt.subplots(figsize=(12, 12))
corr = df.corr().round(2)
sns.heatmap(corr, vmin=-1, vmax=1, cmap=sns.diverging_palette(200, 10, as_cmap=True), square=True, ax=ax, annot = True)

"""Following relations are correlated with some attributes since absolute of correlation values are too close to 1:
*   Price - Payment value (corr: 0.74 --> positive correlation)
*   Product weight - Freight value (corr: 0.61 --> positive correlation)

WeÂ wouldÂ concludeÂ thatÂ asÂ theÂ weightÂ ofÂ theÂ productÂ increases, theÂ freightÂ valueÂ will also increase.

## **Exploratory Data Analysis with Sweetviz**
"""

!pip install sweetviz

import sweetviz as sv

advert_report = sv.analyze(df)

advert_report.show_html('Advertising.html')

"""##**Data Visualization**"""

plt.rcParams['figure.figsize'] = (14,7)

"""**Top 10 states with the most customers**"""

customer_unique = customers_df['customer_unique_id'].nunique()    # Number of customers
customer_unique

"""There are 96096 unique customers."""

state_uniquecus = customers_df['customer_state'].nunique()       # Number of states that customers live
state_uniquecus

"""There are 27 unique states that customers live."""

# Top 10 states with the most customers

piv7 = customers_df.groupby('customer_state')[['customer_unique_id']].nunique().sort_values(by='customer_unique_id', ascending=False) 
piv7 = piv7.rename(columns={'customer_unique_id':'number_of_customers'})
piv7.head(10)

cusstate = customers_df.groupby('customer_state')['customer_unique_id'].nunique().sort_values(ascending=False) 
cusstate10 = cusstate.head(10)

cusstate10.plot(kind='bar', color='slateblue')

plt.title('Top 10 States with the most Customers')
plt.xlabel('Customer State')
plt.ylabel('Number of Unique Customers')
plt.xticks(rotation = 45);

"""We can say that there are 

40302 unique customers in the state Sao Paulo,

12384 unique customers in the state Rio de Janeiro,

11259 unique customers in the state Minas Gerais and so on.

**Top 10 cities with the most customers**
"""

city_uniquecus = customers_df['customer_city'].nunique()   # Number of cities that customers live
city_uniquecus

"""There are 4119 unique cities that customers live."""

piv6 = customers_df.groupby('customer_city')[['customer_unique_id']].nunique().sort_values(by='customer_unique_id',ascending=False) 
piv6 = piv6.rename(columns={'customer_unique_id':'number_of_customers'})
piv6.head(10)

cuscity = customers_df.groupby('customer_city')['customer_unique_id'].nunique().sort_values(ascending=False) 
cuscity10 = cuscity.head(10)

cuscity10.plot(kind='bar', color='seagreen')

plt.title('Top 10 Cities with the most Customers')
plt.xlabel('Customer City')
plt.ylabel('Number of Unique Customers')
plt.xticks(rotation = 60);

"""We can say that  there are

14984 unique customers from Sao Paulo,

6620 unique customers from Rio de Janeiro, 

2672 unique customers from Belo Horizonte and so on.

Also, we can see the top 10 cities with the most customers.

**Top 10 states with the most sellers**
"""

seller_unique = sellers_df['seller_id'].nunique()    # Number of sellers
seller_unique

"""There are 3095 sellers."""

state_uniquesel = sellers_df['seller_state'].nunique()
print('Number of states that sellers live:', state_uniquesel)

city_uniquesel = sellers_df['seller_city'].nunique()
print('Number of cities that sellers live:', city_uniquesel)

piv5 = sellers_df.groupby('seller_state')[['seller_id']].nunique().sort_values(by='seller_id', ascending=False) 
piv5 = piv5.rename(columns={'seller_id':'number_of_sellers'})
piv5.head(10)

selstate = sellers_df.groupby('seller_state')['seller_id'].nunique().sort_values(ascending=False) 
selstate10 = selstate.head(10)

selstate10.plot(kind='bar', color='rosybrown')

plt.title('Top 10 States with the most Sellers')
plt.xlabel('Seller State')
plt.ylabel('Number of Sellers')
plt.xticks(rotation = 45);

"""We can say that there are

1849 sellers from Sao Paulo,

349 sellers from Puerto Rico, 

244 sellers from Minas Gerais and so on.

**Number of orders with respect to payment types**
"""

orders_df['order_id'].nunique()     # Number of orders

notdefp = order_payments_df[~(order_payments_df['payment_type']=='not_defined')]

piv4 = notdefp.pivot_table(values='order_id', index='payment_type', aggfunc=pd.Series.nunique)
piv4 = piv4.rename(columns={'order_id':'number_of_orders'})
piv4.sort_values(by='number_of_orders', ascending=False)

payment_type = notdefp.groupby('payment_type')['order_id'].nunique().sort_values(ascending=False)

colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']
payment_type.plot.pie(autopct="%.2f%%", explode=[0.04]*4, colors=colors)
plt.title('Percentage of orders with respect to payment type', fontsize=14)

my_circle=plt.Circle((0,0), 0.4, color='white')
p=plt.gcf()
p.gca().add_artist(my_circle)
plt.show();

"""We can see that credit card is the most used payment method while debit card is the least used payment method. 

75.24% of the payments made by credit card.

Examine the installments of the payments made with a credit card:
"""

credit = order_payments_df[order_payments_df['payment_type']=='credit_card']

credit['payment_installments'].value_counts().sort_index(ascending=True)

credit.payment_installments.value_counts().sort_index(ascending=True).plot(kind='bar', color='cadetblue')

plt.title('Count of payments with respect to payment installments')
plt.xlabel('Number of Payment Installment')
plt.ylabel('Number of Payments')
plt.xticks(rotation = 45);

"""The maximum payment installment is 24.

We can see that most of the payments made by one installment.

**15 product categories with the highest average of number of installments**
"""

piv10 = df.groupby('product_category_name_english')[['payment_installments']].mean().sort_values(by='payment_installments', ascending=False)
piv10 = piv10.rename(columns={'payment_installments':'average_payment_installments'})
piv10.head(15)

avg_payins = df.groupby('product_category_name_english')['payment_installments'].agg(np.mean).sort_values(ascending=False)
avg_pay15 = avg_payins.head(15)

avg_pay15.plot(kind='barh', color='indianred')

plt.title('15 product categories with the highest average of number of installments')
plt.xlabel('Average Payment Installments')
plt.ylabel('Product Category');

"""We can see that people prefer to increase number of payment installments when they buy computers.

**15 product categories with the lowest average of number of installments**
"""

piv11 = df.groupby('product_category_name_english')[['payment_installments']].mean().sort_values(by='payment_installments', ascending=False)
piv11 = piv11.rename(columns={'payment_installments':'average_payment_installments'})
piv11.tail(15)

avg_payins = df.groupby('product_category_name_english')['payment_installments'].agg(np.mean).sort_values(ascending=False)
avg_pay15low = avg_payins.tail(15)

avg_pay15low.plot(kind='barh', color='mediumpurple')

plt.title('15 product categories with the lowest average of number of installments')
plt.xlabel('Average Payment Installments')
plt.ylabel('Product Category');

"""**15 product categories with the highest average price**"""

piv1 = df.groupby('product_category_name_english')[['price']].mean().sort_values(by='price', ascending=False)
piv1 = piv1.rename(columns={'price':'average_price'})
piv1.head(15)

avg_pr = df.groupby('product_category_name_english')['price'].agg(np.mean).sort_values(ascending=False)
cos_15 = avg_pr.head(15)

cos_15.plot(kind='barh', color='darkcyan')

plt.title('15 product categories with the highest average price')
plt.xlabel('Average Price')
plt.ylabel('Product Category');

"""We can see that the product category with the highest average price is the computers category.

**15 product categories with the lowest average price**
"""

piv1 = df.groupby('product_category_name_english')[['price']].mean().sort_values(by='price', ascending=False)

piv1.tail(15)

avg_pr = df.groupby('product_category_name_english')['price'].agg(np.mean).sort_values(ascending=False)
cos_15low = avg_pr.tail(15)

cos_15low.plot(kind='barh', color='darkkhaki')

plt.title('15 product categories with the lowest average price')
plt.xlabel('Average Price')
plt.ylabel('Product Category');

"""We can see that the product category with the lowest average price is home comfort category.

**Top 15 product category**
"""

category_translations_df.shape    # Number of product category

"""There are 71 product categories."""

piv3 = df.groupby('product_category_name_english')[['order_id']].nunique().sort_values(by='order_id',ascending=False)
piv3 = piv3.rename(columns={'order_id':'number_of_orders'})
piv3.head(15)

top15_prod = df.groupby('product_category_name_english')['order_id'].nunique().sort_values(ascending=False).head(15)

top15_prod.plot(kind='barh', color='coral')

plt.title('Top 15 Product Categories')
plt.xlabel('Number of Orders')
plt.ylabel('Product Category');

"""We see that the most popular product category is the bed bath table.

And we can see the top 15 products.

**Average prices of top 15 product category**
"""

top15cat = df[df['product_category_name_english'].isin(top15_prod.index)]

price_top15_cat = round(top15cat.groupby('product_category_name_english')['price'].agg(np.mean).sort_values(ascending=False), 3)

piv2 = round(top15cat.groupby('product_category_name_english')[['price']].agg(np.mean).sort_values(by='price', ascending=False),3)
piv2 = piv2.rename(columns={'price':'average_price'})
piv2.head(15)

"""Watches category have the max average price within the top 15 product category.

**Price distribution of top 15 product categories**
"""

sns.boxplot(data=top15cat, x='price', y='product_category_name_english',showfliers=False,color='olive')

plt.title('Price distribution of top 15 products')
plt.xlabel('Price')
plt.ylabel('Product Category');

"""When we look at the price distribution we can see that the product category with the largest difference between the cheapest product and the most expensive product is the watch category.

**Examine the relationship between delivery time and review score**
"""

df['delivery_time'] = (df['order_delivered_customer_date']- df['order_approved_at']).dt.days

pearsonr(df['delivery_time'], df['review_score'])

"""When we look at the correlation coefficient and check whether or not the correlation coefficient is statistically significant we get the above results:

the correlation coefficient is -0.32682 

the p-value 0.0

Since the p-value is less than Î± = 0.05, we would conclude that the correlation between delivery time and review score is statistically significant. Also, we can say that there is a negative correlation.
"""

piv9 = df.pivot_table(values='delivery_time', index='review_score').sort_values(by='delivery_time', ascending=False)

piv9

piv9.plot(kind='bar', color='maroon')

plt.title('Review score with respect to delivery time')
plt.xlabel('Review Score')
plt.ylabel('Delivery Time')
plt.xticks(rotation = 0);

"""We can say that as the product delivery time increases the review score decreases.

**Top 3 customers with the highest order amount**
"""

piv14 = df.groupby('customer_unique_id')[['order_id']].nunique().sort_values(by='order_id', ascending=False)

piv14 = piv14.rename(columns={'order_id':'number_of_orders'})

piv14.head(3)

"""**Repeat Customers**"""

len(piv14[piv14['number_of_orders']>2])   # Number of customers that made 2 or more purchases

"""226 customers made 2 or more purchases. Total number of customer is 96096. 

So 99.76% of the customers are single buyer.

**Average order amounts of the 10 customers with the highest number of orders**
"""

dfdro = df.drop_duplicates(subset='order_id', keep='last')

topten = piv14.head(10)

top10customer = dfdro[dfdro['customer_unique_id'].isin(topten.index)]

price_top15_cat = top10customer.groupby('customer_unique_id')[['payment_value']].agg(np.mean).sort_values(by='payment_value', ascending=False)
price_top15_cat = price_top15_cat.rename(columns={'payment_value':'average_order_amount'})

price_top15_cat

price_top15_cat.plot(kind='bar', color='lightsteelblue')

plt.title('Average order amount of top 10 customers')
plt.xlabel('Customer unique id')
plt.ylabel('Average order amount')
plt.xticks(rotation = 80);

"""The average order amounts of the 10 customers with the highest number of orders change between 18 Real and 182 Real.

**Categories that bought by customer with the highest number of orders**
"""

top1cus = df[df['customer_unique_id']=='8d50f5eadf50201ccdcedfb9e2ac8455']['product_category_name_english'].value_counts(normalize=True)

colors = ['#8B636C', '#458B74', '#CD5B45']

top1cus.plot.pie(autopct="%.2f%%", explode=[0.04]*3, colors=colors)

plt.title('Product categories purchased by the 1st customer with the highest amount of orders', fontsize=14)
plt.ylabel('');

"""**Categories that bought by customer with the second highest number of orders**"""

top2cus = df[df['customer_unique_id']=='3e43e6105506432c953e165fb2acf44c']['product_category_name_english'].value_counts(normalize=True)

top2cus.plot.pie(autopct="%.2f%%", explode=[0.04]*5)

plt.title('Product categories purchased by the 2nd customer with the highest amount of orders', fontsize=14)
plt.ylabel('');

"""**Categories that bought by customer with the third highest number of orders**"""

top3cus = df[df['customer_unique_id']=='ca77025e7201e3b30c44b472ff346268']['product_category_name_english'].value_counts(normalize=True)

colors = ['#C0C0C0', '#D1B26F', '#DDA0DD', '#808000']

top3cus.plot.pie(autopct="%.2f%%", explode=[0.04]*4, colors=colors)

plt.title('Product categories purchased by the 3rd customer with the highest amount of orders', fontsize=14)
plt.ylabel('');

"""**Top 10 sellers**"""

piv15 = df.groupby('seller_id')[['order_id']].nunique().sort_values(by='order_id', ascending=False)

piv15 = piv15.rename(columns={'order_id':'number_of_orders'})

piv15.head(10)

"""**Categories that sold by top 3 sellers**"""

top1seller = df[df['seller_id']=='6560211a19b47992c3666cc44a7e94c0'].groupby(['product_category_name_english','order_id']).size().groupby(level=0).count().sort_values(ascending=False)
top2seller = df[df['seller_id']=='4a3ca9315b744ce9f8e9374361493884'].groupby(['product_category_name_english','order_id']).size().groupby(level=0).count().sort_values(ascending=False)
top3seller = df[df['seller_id']=='cc419e0650a3c5ba77189a1882b7556a'].groupby(['product_category_name_english','order_id']).size().groupby(level=0).count().sort_values(ascending=False)

top1seller

top2seller

top3seller

top1seller.plot(kind='bar', color='salmon')

plt.title('Number of orders that sold by seller with the highest amount of order')
plt.xlabel('Product Categories')
plt.ylabel('Number of Orders')
plt.xticks(rotation = 45);

top2seller.plot(kind='bar', color='teal')

plt.title('Number of orders that sold by seller with the second highest amount of order')
plt.xlabel('Product Categories')
plt.ylabel('Number of Orders')
plt.xticks(rotation = 45);

top3seller.plot(kind='bar', color='lightsteelblue')

plt.title('Number of orders that sold by seller with the third highest amount of order')
plt.xlabel('Product Categories')
plt.ylabel('Number of Orders')
plt.xticks(rotation = 45);

"""**Top 10 product categories with the highest amount of review comment**"""

dfwoutnone = df[~(df['review_comment_message']=='NONE')]

dfwoutnone.groupby('product_category_name_english')[['review_comment_message']].nunique().sort_values(by='review_comment_message', ascending=False).head(10)

"""The product category with the highest amount of review comment is bed bath n table.

**Active sellers according to response time for review comments**
"""

df['review_answer_d'] = (df['review_answer_timestamp'] - df['review_creation_date']).dt.days

piv19 = df.groupby('seller_id')[['review_answer_d']].mean().sort_values(by='review_answer_d', ascending=True)

a=piv19[piv19['review_answer_d']==0].count()
b=piv19[piv19['review_answer_d']==1].count()
c=piv19[piv19['review_answer_d']==2].count()
d=piv19[piv19['review_answer_d']==3].count()
e=piv19[piv19['review_answer_d']==4].count()
f=piv19[piv19['review_answer_d']==5].count()

number_of_seller = [194/seller_unique*100,309/seller_unique*100,209/seller_unique*100,102/seller_unique*100,42/seller_unique*100,21/seller_unique*100]
response_time_days = ('0', '1', '2', '3', '4', '5')
y_pos = np.arange(len(response_time_days))

plt.bar(y_pos, number_of_seller, color='lightseagreen')
plt.title('Sellers according to response time')
plt.xlabel('Response time in terms of days')
plt.ylabel('Seller rate')

plt.xticks(y_pos, response_time_days)

plt.show();

"""Approximately 6% of the sellers response customers' comments in less than 1 day. 

10% of the sellers response within 1 day.

**Orders by order status**
"""

order_st = df.groupby('order_status')[['order_id']].nunique().sort_values(by='order_id', ascending=False)
order_st = order_st.rename(columns={'order_id':'order_rate'})
order_st = order_st/order_st['order_rate'].sum()
order_st

order_st.plot(kind='bar', color='orange')

plt.title('Percentage of orders with respect to order status')
plt.xlabel('Order Status')
plt.ylabel('Percentage of Order')
plt.xticks(rotation = 0);

orre = df[df['order_status']=='canceled']

piv20 = orre.groupby('product_category_name_english')[['order_id']].nunique().sort_values(by='order_id', ascending=False).head(15)
piv20 = piv20.rename(columns={'order_id':'number_of_cancelled_orders'})
piv20

"""46 of the purchases made in the sports leisure category have been cancelled.

**Transaction value by year**
"""

tra_year = tra_year.rename(columns={'payment_value':'transaction_value'})
tra_year

tra_year=pd.DataFrame(dfdro.groupby('order_purchase_year')['payment_value'].sum().reset_index())
ax=sns.barplot(x='order_purchase_year',y='payment_value',data=tra_year,palette=sns.set_palette(palette='hls'))

ax.set_xlabel('Year')
ax.set_ylabel('Total Transaction Value')
ax.set_title('Transaction Value by Year')
plt.yscale("linear");
#plt.yscale("log")

"""**Number of transactions by year**"""

ord_year = ord_year.rename(columns={'order_id':'number_of_transaction'})
ord_year

ord_year=pd.DataFrame(dfdro.groupby('order_purchase_year')['order_id'].count().reset_index())
ax=sns.barplot(x='order_purchase_year',y='order_id',data=ord_year,palette=sns.set_palette(palette='afmhot'))

ax.set_xlabel('Year')
ax.set_ylabel('Number of Transactions')
ax.set_title('Number of Transactions by Year');

"""**Number of Customers by Year**"""

df2016['customer_unique_id'].nunique()

df2018['customer_unique_id'].nunique()

cus_year=pd.DataFrame(dfdro.groupby('order_purchase_year')['customer_unique_id'].count().reset_index())
ax=sns.barplot(x='order_purchase_year',y='customer_unique_id',data=cus_year,palette=sns.set_palette(palette='hls'))

ax.set_xlabel('Year')
ax.set_ylabel('Total Number of Customers')
ax.set_title('Total Number of Customers by Year');

"""**Number of Orders by date**"""

mon_or=pd.DataFrame(dfdro.groupby('monthyear')['order_id'].count().reset_index())
ax=sns.barplot(x='monthyear',y='order_id',data=mon_or,palette=sns.set_palette(palette='hls'))

ax.set_xlabel('Date')
ax.set_ylabel('Number of Orders')
ax.set_title('Total Number of Orders by Date')
plt.xticks(rotation=80);

"""When we look at the above graph we can see that there is a significant increase in November. That is because of Black Friday.

**Total transaction value by month**
"""

tra_mon=pd.DataFrame(dfdro.groupby('monthyear')['payment_value'].sum().reset_index())
ax=sns.barplot(x='monthyear',y='payment_value',data=tra_mon,palette=sns.set_palette(palette='Set2'))

ax.set_xlabel('Date')
ax.set_ylabel('Total Transaction Value')
ax.set_title('Transaction Value by Month')
plt.xticks(rotation=70);

"""**Total revenue by region**"""

totrev = dfdro.groupby('customer_state')[['payment_value']].sum()
print('Total revenue:', totrev['payment_value'].sum())
rev1 = dfdro.groupby('customer_state')[['payment_value']].sum().sort_values(by='payment_value', ascending=False).head(5)
rev1.rename(columns={'payment_value':'revenue'})

"""Sao Paulo, Rio de Janeiro and Minas Gerais have 63% of the total revenue.

**Monthly revenue of top 5 product categories**
"""

sales_df=df.groupby(['product_category_name_english'])['price'].sum()
best_sellers=sales_df.nlargest(5).index

best_df=df[df['product_category_name_english'].isin(best_sellers)]

best_monthly=best_df.pivot_table(index='order_purchase_month',columns='product_category_name_english',values='price', aggfunc='sum')
best_monthly.plot(kind='bar',figsize=(18, 12))
plt.title('Top product categories monthly revenue')
plt.xlabel('month')
plt.ylabel('total revenue');

"""In May watches category generated most money.

In August health beauty category generated most money.

**Top 5 product categories that generating most money**
"""

prorev = dfdro.groupby('product_category_name_english')[['payment_value']].sum().sort_values(by='payment_value', ascending=False).head(5)
prorev.rename(columns={'payment_value':'revenue'})

prorev.plot(kind='bar', color='darkcyan')

plt.title('Top 5 product categories that generating most money')
plt.xlabel('Product Category')
plt.ylabel('Revenue')
plt.xticks(rotation = 50)
plt.yscale('log');

"""Health beauty category generated the most money among all categories.

**Review score distribution**
"""

p_5s = len(order_reviews_df[order_reviews_df['review_score'] == 5])*100/len(order_reviews_df)
p_1s = len(order_reviews_df[order_reviews_df['review_score'] == 1])*100/len(order_reviews_df)
avg_s = order_reviews_df['review_score'].mean()

print(len(order_reviews_df), 'reviews')
print(f'5â˜…: {p_5s:.1f}%')
print(f'1â˜…: {p_1s:.1f}%')
print(f'Average: {avg_s:.1f}â˜…')

sns.catplot(
 x='review_score',
 kind='count', 
 data=order_reviews_df,
 palette='hls'
).set(
 xlabel='Review Score',
 ylabel='Number of Reviews',
);

"""57.8% of the given scores is 5,

11.5% of the given scores is 1.

The average score is 4.1

###**Conclusion**

1. 99.76% of the customers is single buyer. Considering the '80% of future profits will come from just 20% of existing customers', it is important to keep repeat customers. Customer loyalty programs might be created or discounts and incentives might be offered to customers to attract to make purchase again.

2. It is important to response customers' questions or reviews in a short time. So a quick response rate might be maintained.

3. Landing pages could be created for top categories or the categories that generating the most money. Promotions and recommendations could be provided for the best-selling and trending products to customers in the states of Sao Paulo, Rio de Janeiro, Minas Gerais, Rio Grande do Sul and Parana because 77% of customers are in that state.

4. Customers take into account the review scores when they make purchases. Review score decreases as delivery time increases. So delivery performance could be improved.

5. As the number of reviews of products increases, it attracts the attention of customers. So the products with most comments could be highlighted.

6. The average order amounts of customers with the highest number of orders change between 18 and 182 Real. So cross sell could be created or bundle products could be created and discounts could be offered for bundle products to increase average order value.

##**Future Work**

1. Customer segmentation might be done based on the duration since the date of last purchase, the total count of purchases and the average amount purchased (RFM Technique). So that their promotional strategy and recommendations are more optimal, their target markets can be more precise, and of course it saves more time and marketing costs.

2.    Sentiment analysis in product reviews using Natural Language Processing (NLP) could be performed and this helps to properly discern the customer different preferences, likes, dislikes, etc. These extracted insights can then be used to improve customer service and experience.
"""